# 异常检测算法模型
> 数据集：用的是鼠标移动轨迹的3000条带标签的数据
## 1.多种聚类模型结合形式
* （1）KMeans：利用找到的簇的中心点和每一个样本的距离值，找到最偏离簇中心的点作为异常点。
* （2）DBSCAN--基于密度的聚类：由于需要涉及到算法本身两个参数（min_samples和eps），这里模型会直接输出超过半径eps和确定好最小数的min_samples的样本点作为异常值（label = -1）。
* （3）Birch--基于层次的聚类：BIRCH算法利用了一个树结构来帮助我们快速的聚类，这个数结构类似于平衡B+树，一般将它称之为聚类特征树(Clustering Feature Tree，简称CF Tree)。建立好CF Tree后把那些包含数据点少的MinCluster当作outlier。
> （1）利用三个聚类模型输出的异常点求其并集，一共输出了309个异常点，真正的异常点有400个，TP有193个，因此：P = 0.625，R = 0.4825，F1_score = 0.5445
> （2）这种方式只能通过拿到全部数据，对所有数据进行离线聚类的形式才能得到异常点，因此，无法做到在线学习的过程。

## 2.IsolationForest孤立森林
* 1. 孤立森林的思想是：
```
假设我们用一个随机超平面来切割（split）数据空间（data space）, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。
```
* 2. 孤立森林的优势：
```
孤立森林算法具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。
```
* 3. 孤立森林的劣势：
```
孤立森林不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。
```
> （1）结果：P = 0.69，R = 0.49，F1_score = 0.57
> （2）这种方式可以通过训练的方式找到一个正常样本在特征空间中的区域（通过对正常样本的描述，），对于不在这个区域中的样本，视为异常。因此可以直接在线输出结果。

## 3.One-class SVM
* （1）与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。
![oneclassSVM](markdown_img\oneclassSVM.png)
* （2）与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。
![oneclassSVM_2](markdown_img\oneclassSVM_2.jpg)
> （1）结果：P = 0.48，R = 0.94，F1_score = 0.63
> （2）这种方式可以通过训练的方式找到类的边界，而不是最大间隔，因此可以通过学习到的边界，来在线定义是否为异常。
